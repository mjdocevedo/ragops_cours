# RAGOPS - Production-Ready RAG Pipeline

[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://docker.com)
[![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)](https://fastapi.tiangolo.com)
[![Meilisearch](https://img.shields.io/badge/Meilisearch-Search-orange)](https://meilisearch.com)
[![LiteLLM](https://img.shields.io/badge/LiteLLM-Proxy-purple)](https://litellm.ai)

A production-ready Retrieval-Augmented Generation (RAG) pipeline built with modern technologies, designed for CPU deployment with enterprise-grade features.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RAGOPS ARCHITECTURE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”‚   Nginx     â”‚â”€â”€â”€â”€â”‚  FastAPI    â”‚â”€â”€â”€â”€â”‚ Meilisearch â”‚
â”‚ Application â”‚    â”‚  (Optional) â”‚    â”‚  Backend    â”‚    â”‚   Search    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚                    â”‚
                                              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚              â”‚ Document â”‚
                                              â”‚              â”‚ & Chunks â”‚
                                              â”‚              â”‚ Indexes  â”‚
                                              â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Redis     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  LiteLLM    â”‚
                    â”‚  Caching    â”‚          â”‚          â”‚   Proxy     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚                    â”‚
                                              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚              â”‚   Groq   â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   LLM    â”‚
                                                            â”‚ Provider â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚     TEI     â”‚                               â”‚
                    â”‚ Embeddings  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  Service    â”‚          
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          

Flow:
1. Documents â†’ Ingestion â†’ Chunking â†’ Embeddings â†’ Meilisearch
2. Query â†’ FastAPI â†’ Meilisearch (Hybrid Search) â†’ Context â†’ LLM â†’ Response
3. Redis caches embeddings and responses for performance
```

## ğŸš€ Key Features

### Core Capabilities
- **ğŸ” Hybrid Search**: Combines vector similarity and BM25 text search
- **ğŸ“„ Document Processing**: Supports multiple document types with intelligent chunking
- **ğŸ§  LLM Integration**: Groq LLMs via LiteLLM proxy with fallback support
- **âš¡ High Performance**: Redis caching with 5-50x speed improvements
- **ğŸ¯ Semantic Retrieval**: TEI embeddings for semantic understanding
- **ğŸ”§ Production Ready**: Docker Compose orchestration with health checks

### Technical Features
- **CPU Optimized**: Runs efficiently on CPU-only infrastructure
- **Scalable Architecture**: Microservices design with independent scaling
- **Enterprise Security**: Authentication, authorization, and secure communication
- **Monitoring & Logging**: Comprehensive observability stack
- **API Documentation**: Auto-generated OpenAPI/Swagger documentation

## ğŸ“‹ Prerequisites

- **Docker & Docker Compose**: Latest versions
- **4GB+ RAM**: Recommended for optimal performance
- **API Keys**: Groq API key for LLM access
- **Storage**: 2GB+ free disk space for models and indexes

## âš¡ Quick Start

### 1. Clone and Configure

```bash
git clone <repository-url>
cd RAGOPS

# Copy and configure environment
cp .env.example .env
# Edit .env with your API keys (see Configuration section)
```

### 2. Start the Stack

```bash
# Start all services
docker compose up -d

# Check service health
docker compose ps

# View logs
docker compose logs -f
```

### 3. Verify Installation

```bash
# Check API health
curl http://localhost:18000/health

# Access API documentation
open http://localhost:18000/docs

# Run system validation
docker compose exec backend python final_rag_test_report.py
```

### 4. Ingest Sample Documents

```bash
# Ingest sample documents for testing
docker compose exec backend python ingest.py

# Or ingest your own documents via API
curl -X POST "http://localhost:18000/ingest" \
  -H "Content-Type: application/json" \
  -d '[{"id": "doc1", "text": "Your document content", "metadata": {"source": "file.pdf"}}]'
```

### 5. Test RAG Queries

```bash
# Test search and generation
curl -X POST "http://localhost:18000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is this document about?", "k": 3}'

# Test direct chat
curl -X POST "http://localhost:18000/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello!"}]}'
```

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Meilisearch Configuration
MEILI_KEY=your_secure_master_key_here
MEILI_INDEX=documents
EMBED_DIM=384

# LLM Provider Configuration  
LITELLM_KEY=your_proxy_key_here
GROQ_API_KEY=your_groq_api_key_here

# Optional: Additional LLM providers
OPENAI_API_KEY=your_openai_key_here
HUGGINGFACE_API_KEY=your_hf_key_here

# Service URLs (Docker internal)
MEILI_URL=http://meilisearch:7700
PROXY_URL=http://litellm:4000
REDIS_URL=redis://redis:6379
```

### LiteLLM Model Configuration

Edit `litellm/config.yaml` to customize:

```yaml
model_list:
  # Primary chat model
  - model_name: groq-llama3
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: os.environ/GROQ_API_KEY

  # Local embeddings
  - model_name: local-embeddings
    litellm_params:
      model: openai/text-embedding-ada-002
      api_base: "http://tei-embeddings:80"
      api_key: "dummy-key"

# Global settings
litellm_settings:
  cache: true
  cache_params:
    type: "redis"
    url: "redis://redis:6379"
    ttl: 1800
```

## ğŸ“Š Service Architecture

### Core Services

| Service | Port | Description | Health Check |
|---------|------|-------------|--------------|
| **FastAPI Backend** | 18000 | Main API server | `GET /health` |
| **Meilisearch** | 7700 | Search & vector database | `GET /health` |
| **LiteLLM Proxy** | 4000 | LLM routing proxy | `GET /health` |
| **TEI Embeddings** | 80 | Text embeddings service | `GET /health` |
| **Redis** | 6379 | Caching layer | TCP check |
| **Nginx** | 8443 | Reverse proxy (optional) | HTTP check |

### Data Flow

1. **Document Ingestion**:
   ```
   Documents â†’ FastAPI â†’ Processing â†’ Embeddings (TEI) â†’ Meilisearch
   ```

2. **Query Processing**:
   ```
   Query â†’ FastAPI â†’ Embeddings (TEI) â†’ Search (Meilisearch) â†’ Context â†’ LLM (Groq) â†’ Response
   ```

3. **Caching Layer**:
   ```
   Redis caches: Embeddings (1h TTL) | LLM Responses (10min TTL)
   ```

## ğŸ“š API Documentation

### Core Endpoints

#### Document Management
```http
POST /ingest
Content-Type: application/json

[
  {
    "id": "doc-1",
    "text": "Document content here",
    "metadata": {"source": "file.pdf", "author": "John Doe"}
  }
]
```

#### Search & Retrieval
```http
POST /search
Content-Type: application/json

{
  "query": "What is machine learning?",
  "k": 5
}

Response:
{
  "answer": "Machine learning is...",
  "chunks": [...],
  "total_chunks_found": 10,
  "cached": false
}
```

#### Direct Chat
```http
POST /chat
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Explain quantum computing"}
  ],
  "temperature": 0.3,
  "model": "groq-llama3"
}
```

#### System Status
```http
GET /health           # API health
POST /init-index      # Initialize search indexes
```

### Interactive Documentation

- **Swagger UI**: http://localhost:18000/docs
- **ReDoc**: http://localhost:18000/redoc

## ğŸ§ª Testing & Validation

### Automated Testing

```bash
# Run comprehensive system validation
docker compose exec backend python final_rag_test_report.py

# Demo working features
docker compose exec backend python demo_working_features.py

# Manual ingestion test
docker compose exec backend python ingest.py
```

### Performance Benchmarks

The system has been validated with:
- **API Response Time**: 3-9ms average
- **Cache Performance**: 5-51x speedup with Redis
- **Document Processing**: Supports documents from 50 to 5000+ words
- **Concurrent Requests**: Handles multiple simultaneous queries
- **Search Accuracy**: Hybrid search with relevance scoring

### Health Monitoring

```bash
# Check all services
docker compose ps

# View service logs
docker compose logs [service-name]

# Monitor resource usage
docker stats

# Test individual components
curl http://localhost:7700/health    # Meilisearch
curl http://localhost:18000/health   # FastAPI Backend
```

## ğŸ”§ Development & Customization

### Project Structure

```
RAGOPS/
â”œâ”€â”€ docker-compose.yml          # Service orchestration
â”œâ”€â”€ .env                       # Environment configuration
â”œâ”€â”€ backend/                   # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ main.py           # Main API application
â”‚   â”œâ”€â”€ Dockerfile            # Backend container
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ ingest.py             # Sample data ingestion
â”‚   â”œâ”€â”€ demo_working_features.py  # Feature demonstration
â”‚   â””â”€â”€ final_rag_test_report.py  # System validation
â”œâ”€â”€ litellm/
â”‚   â””â”€â”€ config.yaml           # LLM proxy configuration
â””â”€â”€ nginx/                    # Optional reverse proxy
    â””â”€â”€ nginx.conf
```

### Adding Custom Documents

```python
# Via API
import httpx

documents = [
    {
        "id": "custom-doc-1",
        "text": "Your document content here...",
        "metadata": {
            "title": "Document Title",
            "author": "Author Name",
            "category": "technical",
            "tags": ["ai", "machine-learning"]
        }
    }
]

async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:18000/ingest",
        json=documents
    )
    print(response.json())
```

### Extending LLM Providers

Add new providers in `litellm/config.yaml`:

```yaml
model_list:
  # OpenAI GPT-4
  - model_name: openai-gpt4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY

  # Anthropic Claude
  - model_name: claude-3
    litellm_params:
      model: anthropic/claude-3-sonnet
      api_key: os.environ/ANTHROPIC_API_KEY
```

## ğŸ“ˆ Production Deployment

### Scaling Considerations

1. **Horizontal Scaling**:
   ```yaml
   # In docker-compose.yml
   backend:
     deploy:
       replicas: 3
   
   redis:
     deploy:
       replicas: 1  # Redis should remain single instance
   ```

2. **Resource Allocation**:
   ```yaml
   services:
     backend:
       deploy:
         resources:
           limits:
             memory: 2G
             cpus: '1.0'
   ```

3. **Data Persistence**:
   ```yaml
   volumes:
     meili_data:
       driver: local
       driver_opts:
         type: none
         o: bind
         device: /data/meilisearch
   ```

### Security Hardening

1. **Environment Security**:
   ```bash
   # Use strong, unique keys
   MEILI_KEY=$(openssl rand -hex 32)
   LITELLM_KEY=$(openssl rand -hex 32)
   
   # Restrict network access
   # Configure firewall rules
   # Use TLS certificates
   ```

2. **API Security**:
   - Enable authentication in LiteLLM config
   - Configure rate limiting
   - Set up request validation
   - Monitor API access logs

### Monitoring Setup

```yaml
# Add to docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

## ğŸ” Troubleshooting

### Common Issues

1. **Service Won't Start**:
   ```bash
   # Check logs
   docker compose logs [service-name]
   
   # Verify environment
   docker compose config
   
   # Restart services
   docker compose restart [service-name]
   ```

2. **Search Not Working**:
   ```bash
   # Check Meilisearch indexes
   curl -H "Authorization: Bearer $MEILI_KEY" \
        http://localhost:7700/indexes
   
   # Reinitialize indexes
   curl -X POST http://localhost:18000/init-index
   ```

3. **LLM Errors**:
   ```bash
   # Verify API keys
   docker compose exec backend env | grep -E "(GROQ|OPENAI)_API_KEY"
   
   # Test LiteLLM directly
   docker compose logs litellm
   ```

4. **Performance Issues**:
   ```bash
   # Check resource usage
   docker stats
   
   # Monitor cache hit rates
   docker compose exec backend python demo_working_features.py
   
   # Clear Redis cache
   docker compose exec redis redis-cli FLUSHALL
   ```

### Debug Commands

```bash
# Access service containers
docker compose exec backend bash
docker compose exec meilisearch sh

# Check network connectivity
docker compose exec backend ping meilisearch
docker compose exec backend ping litellm

# View detailed logs
docker compose logs -f --tail=100

# Restart problematic services
docker compose restart backend litellm
```

## ğŸ“– Additional Resources

### Documentation
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Meilisearch Documentation](https://docs.meilisearch.com/)
- [LiteLLM Documentation](https://docs.litellm.ai/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

### Community
- [Issues & Bug Reports](https://github.com/your-repo/issues)
- [Feature Requests](https://github.com/your-repo/discussions)
- [Contributing Guidelines](CONTRIBUTING.md)

### Deployment Examples
- [AWS ECS Deployment](docs/aws-ecs.md)
- [Kubernetes Deployment](docs/kubernetes.md)
- [Docker Swarm Deployment](docs/docker-swarm.md)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request
