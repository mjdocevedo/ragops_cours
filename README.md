# RAGOPS - Production-Ready RAG Pipeline

[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://docker.com)
[![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)](https://fastapi.tiangolo.com)
[![Meilisearch](https://img.shields.io/badge/Meilisearch-Search-orange)](https://meilisearch.com)
[![LiteLLM](https://img.shields.io/badge/LiteLLM-Proxy-purple)](https://litellm.ai)

A production-ready Retrieval-Augmented Generation (RAG) pipeline built with modern technologies, designed for CPU deployment with enterprise-grade features.

## ğŸ—ï¸ Architecture Overview

```txt
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RAGOPS ARCHITECTURE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”‚   Nginx     â”‚â”€â”€â”€â”€â”‚  FastAPI    â”‚â”€â”€â”€â”€â”‚ Meilisearch â”‚
â”‚ Application â”‚    â”‚  (Optional) â”‚    â”‚  Backend    â”‚    â”‚   Search    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚                    â”‚
                                              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚              â”‚ Document â”‚
                                              â”‚              â”‚ & Chunks â”‚
                                              â”‚              â”‚ Indexes  â”‚
                                              â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Redis     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  LiteLLM    â”‚
                    â”‚  Caching    â”‚          â”‚          â”‚   Proxy     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚                    â”‚
                                              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚              â”‚   Groq   â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   LLM    â”‚
                                                            â”‚ Provider â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚     TEI     â”‚                               â”‚
                    â”‚ Embeddings  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  Service    â”‚          
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          

Flow:
1. Documents â†’ Ingestion â†’ Chunking â†’ Embeddings â†’ Meilisearch
2. Query â†’ FastAPI â†’ Meilisearch (Hybrid Search) â†’ Context â†’ LLM â†’ Response
3. Redis caches embeddings and responses for performance
```

## ğŸš€ Key Features

### Core Capabilities
- **ğŸ” Hybrid Search**: Combines vector similarity and BM25 text search
- **ğŸ“„ Document Processing**: Supports multiple document types with intelligent chunking
- **ğŸ§  LLM Integration**: Groq LLMs via LiteLLM proxy with fallback support
- **âš¡ High Performance**: Redis caching with 5-50x speed improvements
- **ğŸ¯ Semantic Retrieval**: TEI embeddings for semantic understanding
- **ğŸ”§ Production Ready**: Docker Compose orchestration with health checks

### Technical Features
- **CPU Optimized**: Runs efficiently on CPU-only infrastructure
- **Scalable Architecture**: Microservices design with independent scaling
- **Enterprise Security**: Authentication, authorization, and secure communication
- **Monitoring & Logging**: Comprehensive observability stack
- **API Documentation**: Auto-generated OpenAPI/Swagger documentation

## ğŸ“‹ Prerequisites

- **Docker & Docker Compose**: Latest versions
- **4GB+ RAM**: Recommended for optimal performance
- **API Keys**: Groq API key for LLM access
- **Storage**: 2GB+ free disk space for models and indexes

## âš¡ Quick Start

### 1. Clone and Configure

```bash
git clone <repository-url>
cd RAGOPS

# Copy and configure environment
cp .env.example .env
# Edit .env with your API keys (see Configuration section)
```

### 2. Start the Stack

```bash
# Start all services
docker-compose up -d

# Check service health
docker-compose ps

# View logs
docker-compose logs -f
```

### 3. Verify health

```bash
# Check API health
curl http://localhost:18000/health
```

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Meilisearch Configuration
MEILI_KEY=your_secure_master_key_here
MEILI_INDEX=documents
EMBED_DIM=384

# LLM Provider Configuration  
LITELLM_KEY=your_proxy_key_here
GROQ_API_KEY=your_groq_api_key_here

# Optional: Additional LLM providers
OPENAI_API_KEY=your_openai_key_here
HUGGINGFACE_API_KEY=your_hf_key_here

# Service URLs (Docker internal)
MEILI_URL=http://meilisearch:7700
PROXY_URL=http://litellm:4000
REDIS_URL=redis://redis:6379
```

### LiteLLM Model Configuration

Edit `litellm/config.yaml` to customize:

```yaml
model_list:
  # Primary chat/completions model on Groq
  - model_name: groq-llama3
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY

  # Local embeddings served by TEI (OpenAI-compatible embeddings API)
  - model_name: local-embeddings
    litellm_params:
      model: openai/text-embedding-ada-002 
      api_key: os.environ/GROQ_API_KEY
      api_base: "http://tei-embeddings:80"
      custom_llm_provider: openai
      timeout: 60

# Global LiteLLM settings
litellm_settings:
  cache: true
  cache_params:
    type: "redis"
    url: "redis://redis:6379"
    ttl: 1800
    supported_call_types: ["completion", "chat_completion", "embedding", "acompletion", "aembedding"]

# Prompt Injection basic guards
prompt_injection_params:
  heuristics_check: true
  similarity_check: false
  vector_db_check: false

# Routing / fallbacks
router_settings:
  fallbacks:
    - "groq-llama3": []
```

## ğŸ“Š Service Architecture

### Core Services

| Service | Port | Description | Health Check |
|---------|------|-------------|--------------|
| **FastAPI Backend** | 18000 | Main API server | `GET /health` |
| **Meilisearch** | 7700 | Search & vector database | `GET /health` |
| **LiteLLM Proxy** | 4000 | LLM routing proxy | `GET /health` |
| **TEI Embeddings** | 80 | Text embeddings service | `GET /health` |
| **Redis** | 6379 | Caching layer | TCP check |

### Data Flow

1. **Document Ingestion**:
   ```
   Documents â†’ FastAPI â†’ Processing â†’ Embeddings (TEI) â†’ Meilisearch
   ```

2. **Query Processing**:
   ```
   Query â†’ FastAPI â†’ Embeddings (TEI) â†’ Search (Meilisearch) â†’ Context â†’ LLM (Groq) â†’ Response
   ```

3. **Caching Layer**:
   ```
   Redis caches: Embeddings (1h TTL) | LLM Responses (10min TTL)
   ```

## ğŸ§ª Testing & Validation
### Health Monitoring

```bash
# Check all services
docker-compose ps

# View service logs
docker-compose logs [service-name]

# Monitor resource usage
docker stats

# Test individual components
curl http://localhost:7700/health    # Meilisearch
curl http://localhost:18000/health   # FastAPI Backend
```

## ğŸ”§ Development & Customization

### Project Structure

```sh
.
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py
â”‚   â”‚   â”‚   â””â”€â”€ stats.py
â”‚   â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”‚   â”œâ”€â”€ clients.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ responses.py
â”‚   â”‚   â”‚   â””â”€â”€ search.py
â”‚   â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â”‚   â””â”€â”€ search_service.py
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ cache.py
â”‚   â”‚       â””â”€â”€ hashing.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ seed_data.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ litellm
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ pdf_files
â”‚   â”œâ”€â”€ autoencoders.pdf
â”‚   â”œâ”€â”€ linear_algebra.pdf
â”‚   â””â”€â”€ linear_factor_models.pdf
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ meili-init.sh
â””â”€â”€ tests
    â”œâ”€â”€ chunking_validation.py
    â”œâ”€â”€ debug_vector_search.py
    â”œâ”€â”€ demo_phase2.py
    â”œâ”€â”€ demo_working_features.py
    â”œâ”€â”€ final_rag_test_report.py
    â”œâ”€â”€ test_all_features.py
    â”œâ”€â”€ test_direct_ingest.py
    â””â”€â”€ test_phase2_comprehensive.py
```

## ğŸ“ˆ Production Deployment

### Scaling Considerations

1. **Horizontal Scaling**:
   ```yaml
   # In docker-compose.yml
   backend:
     deploy:
       replicas: 3
   
   redis:
     deploy:
       replicas: 1  # Redis should remain single instance
   ```

2. **Resource Allocation**:
   ```yaml
   services:
     backend:
       deploy:
         resources:
           limits:
             memory: 2G
             cpus: '1.0'
   ```

3. **Data Persistence**:
   ```yaml
   volumes:
     meili_data:
       driver: local
       driver_opts:
         type: none
         o: bind
         device: /data/meilisearch
   ```
- - -
# Next Evolution: Advanced Document Processing and Search Enhancement

## ğŸ¯ Overview

Phase 3 will extend RAGOPS with advanced document processing capabilities and search result reranking to create a comprehensive enterprise-grade RAG system.

### Current Status
- âœ… **Phase 1**: Text-based chunking and ingestion
- âœ… **Phase 2**: Embeddings integration with semantic search
- ğŸ¯ **Phase 3**: PDF processing + reranking (this document)

---

## ğŸ—ï¸ Architecture Enhancement

### New Components
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Upload    â”‚â”€â”€â”€â–¶â”‚   LangChain  â”‚â”€â”€â”€â–¶â”‚   Existing     â”‚
â”‚   Interface     â”‚    â”‚   Processor  â”‚    â”‚   Pipeline     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Cross-      â”‚
                       â”‚  Encoder     â”‚
                       â”‚  Reranker    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Enhanced Data Flow
1. **PDF Upload** â†’ LangChain PyPDFLoader â†’ Page extraction
2. **Text Processing** â†’ RecursiveCharacterTextSplitter â†’ Smart chunking
3. **Metadata Enrichment** â†’ Page numbers, file info, structure
4. **Existing Pipeline** â†’ Embeddings â†’ Meilisearch storage
5. **Enhanced Search** â†’ Initial retrieval â†’ Cross-encoder reranking â†’ Final results

---

## ğŸ¯ Quick Start

### 1. Start the System
```bash
make up
```
*This builds images, starts all services, and waits for readiness*

### 2. Validate Installation
```bash
make test
```
*Runs comprehensive Phase 2 validation suite*

### 3. Try a Demo
```bash
make demo
```
*Interactive demonstration of key features*

### 4. Check All Features
```bash
make validate
```
*Complete system validation and feature testing*

---

## ğŸ› ï¸ Available Commands

Run `make help` to see all available commands:

```bash
make help
```

### Core Operations
- `make up` - Start all RAGOPS services
- `make down` - Stop all services  
- `make restart` - Restart all services
- `make logs` - Show backend service logs
- `make clean` - Clean up Docker resources

### Testing & Validation
- `make test` - Run Phase 2 comprehensive tests
- `make demo` - Run feature demonstrations  
- `make validate` - Validate all system features

### Development
- `make dev-logs` - Follow all service logs
- `make dev-rebuild` - Rebuild and restart backend only
- `make dev-reset` - Complete system reset with fresh data

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â”€â”€â”€â–¶â”‚   Backend    â”‚â”€â”€â”€â–¶â”‚  Meilisearch   â”‚
â”‚   (Future)      â”‚    â”‚   FastAPI    â”‚    â”‚   + Vector     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                       â”‚
                              â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   LiteLLM    â”‚    â”‚     Redis      â”‚
                       â”‚   Proxy      â”‚    â”‚    Cache       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     TEI      â”‚    â”‚     Groq       â”‚
                       â”‚ Embeddings   â”‚    â”‚     LLM        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Services Overview
- **Backend** (Port 18000): FastAPI with Phase 2 embeddings
- **Meilisearch** (Port 7700): Vector-enabled search engine
- **TEI-Embeddings** (Port 8080): Text embeddings inference
- **LiteLLM** (Port 4000): Multi-provider LLM proxy
- **Redis** (Port 6379): Embedding cache layer
- **Meili-Init**: Automated index configuration

---

## ğŸ“¡ API Reference

### Core Endpoints

#### Health Check
```bash
curl -s http://localhost:18000/health | jq .
```
**Response**:
```json
{
  "status": "healthy",
  "embeddings_available": true,
  "embedding_dimensions": 384
}
```

#### Document Ingestion
```bash
curl -X POST "http://localhost:18000/ingest" \
  -H "Content-Type: application/json" \
  -d '[{
    "id": "doc1",
    "text": "Your document content here...",
    "metadata": {"title": "Document Title", "category": "docs"}
  }]' | jq .
```

#### Semantic Search & RAG
```bash
curl -X POST "http://localhost:18000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are vector embeddings?", "k": 5}' | jq .
```

#### Test Embeddings
```bash
curl -X POST "http://localhost:18000/test-embeddings" \
  -H "Content-Type: application/json" \
  -d '["text to embed", "another text"]' | jq .
```

#### Initialize Indexes
```bash
curl -X POST "http://localhost:18000/init-index"
```

---

## ğŸ”§ Configuration

### Environment Setup
Create a `.env` file with your configuration:

```bash
# Required: Groq API Key
GROQ_API_KEY=your_groq_api_key_here

# Meilisearch Configuration  
MEILI_KEY=change_me_master_key

# Performance Settings (optional)
REDIS_CACHE_TTL=3600
MAX_CHUNK_SIZE=500
CHUNK_OVERLAP=50
```

### Service Configuration
All service URLs are automatically configured for Docker Compose:
- `PROXY_URL=http://litellm:4000`
- `MEILISEARCH_URL=http://meilisearch:7700`
- `REDIS_URL=redis://redis:6379`
- `TEI_URL=http://tei-embeddings:8080`

---

## ğŸ§ª Testing & Validation

### Using Makefile Commands

#### Quick Validation
```bash
make test
```
*Runs comprehensive Phase 2 test suite*

#### Full System Validation
```bash
make validate  
```
*Tests all features and generates detailed reports*


### Test Files Organization
All tests are in the `tests/` directory:
- `tests/test_phase2_comprehensive.py` - Complete Phase 2 validation
- `tests/test_all_features.py` - Comprehensive feature testing
- `tests/chunking_validation.py` - Text chunking validation

---

#### Check System Status
```bash
# After make up, check services
docker compose ps

# Check resource usage
docker stats
```

### Maintenance Tasks

#### Clean Docker Resources
```bash
make clean
```
*Removes containers, volumes, and prunes system*

#### Manual Cache Clear
```bash
docker compose exec redis redis-cli FLUSHALL
```

#### Backup Data
```bash
# Backup documents
curl -H "Authorization: Bearer $MEILI_KEY" \
  "http://localhost:7700/indexes/documents/documents" > backup_documents.json

# Backup chunks  
curl -H "Authorization: Bearer $MEILI_KEY" \
  "http://localhost:7700/indexes/chunks/documents" > backup_chunks.json
```
